dag:
  dag_id: 'dag_u_ids_pp_trusted' # nombre del DAG
  default_args:
    owner: 'airflow'
    start_date: 'eval: datetime.now() - timedelta(days=1)'
    depends_on_past: False
    email: ['jose.nunez@digitalfemsa.com']
    email_on_failure: True
    email_on_retry: False
    retries: 0
  schedule:
  logical_execution_date: '{{ macros.ds_format(next_ds, "%Y-%m-%d", "%Y/%m/%d") }}'
  run_id: '{{ run_id }}'
  flow_steps: [
    'ingestion_raw_dataproc'
  ]


projects:
  compute_id: 'eval: os.getenv("PROJECT_ID_COMPUTE")'
  landing_id: 'eval: os.getenv("PROJECT_ID_LANDING")'
  raw_id: 'eval: os.getenv("PROJECT_ID_RAW")'
  trusted_id: 'eval: os.getenv("PROJECT_ID_TRUSTED")'


gcp:
  network: 'eval: os.getenv("GENERAL_NETWORK")'
  subnetwork: 'eval: os.getenv("GENERAL_DP_SUBNETWORK01")'

storage:
  landing:
    data:
      bucket: 'eval: os.getenv("GENERAL_LANDING_BUCKET")'
      bucket_file: 'test_u_ids_pp'
      folder: 'reporte_clientes' # donde se guardan los archivos de clientes
  compute:
    temp:
      bucket: 'eval: os.getenv("TEMPORAL_BUCKET")'
      folder: 'test_u_ids_pp' # donde se guardan archivos temporales que genera el proceso
    scripts:
      bucket: 'eval: os.getenv("GENERAL_SCRIPTS_BUCKET")'
    trans_templates:
      bucket: 'eval: os.getenv("TRANSVERSAL_TEMPLATES_DF_BUCKET")'


big_query:
  raw:
    data:
      dataset_id: 'test_u_ids_pp'
      table_id: 'reporte_clientes' # tabla en Raw
  trusted:
    data:
      dataset_id: 'test_u_ids_pp'
      table_id: 'reporte_clientes' # tabla en Trusted


dataproc_sl: 
  service_account_name: 'eval: os.getenv("GENERAL_DP_SERVICE_ACCOUNT")'
  ext_raw:
    retries: 0
    region: 'us-east1'
    project_id: '{self.projects.compute_id}'
    batch_id: 'eval: ("job-read-csv-" + datetime.now().strftime("%y%m%d-%H%M")).replace("_","-")' # nombre del task
    file_name: 'customers.csv' # archivo de clientes
    batch:
      pyspark_batch:
        main_python_file_uri: 'gs://{self.storage.compute.scripts.bucket}/config/dataproc/scripts/test_u_ids_pp/trusted_reporte_clientes.py' # script de python que lee el archivo
        jar_file_uris: ['gs://{self.storage.compute.scripts.bucket}/config/dataproc/lib/delta-spark_2.13-3.2.0.jar']
        args: [
          '{self.storage.landing.data.bucket_file}',  # 1: bucket del archivo
          '{self.storage.landing.data.folder}',  # 2: folder del archivo
          '{self.dataproc_sl.ext_raw.file_name}',  # 3: archivo de clientes
          '{self.storage.compute.temp.bucket}',  # 4: bucket temporal
          '{self.storage.compute.temp.folder}',  # 5: folder temporal
          '{self.projects.raw_id}', # 6: id del projecto en Raw
          '{self.big_query.raw.data.dataset_id}',  # 7: dataset en Raw
          '{self.big_query.raw.data.table_id}',  # 8: table en Raw
          '{self.projects.trusted_id}', # 9: id del projecto en Trusted
          '{self.big_query.trusted.data.dataset_id}',  # 10: dataset en Trusted
          '{self.big_query.trusted.data.table_id}',  # 11: table en Trusted
        ]
      runtime_config:

        properties: [['spark.jars.packages','io.delta:delta-spark_2.13:3.2.0']]
        version: '2.2'
      environment_config:
        execution_config:
          service_account: '{self.dataproc_sl.service_account_name}'
          subnetwork_uri: '{self.gcp.subnetwork}'
          network_tags: 'dataproc'


